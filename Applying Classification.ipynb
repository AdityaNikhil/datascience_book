{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Classification\n",
    "\n",
    "In this chapter we'll look at a few examples of applying classification.  We'll start by looking at A/B testing like we did for applying hypothesis testing.  Then we'll move onto an example with customer churn where the data is unbalanced.  We'll end the chapter with an example of credit card fraud.  \n",
    "\n",
    "## A/B Testing\n",
    "\n",
    "Recall from the Applying Statistical Tests chapter we want to send an email about a towel sale.  We will have both a control and test group - one in which something about the email changed (test) and one in which the email stays the same as in the past (control).  We will use these two samples to set up an experiment.  Did changing the email effect things?\n",
    "\n",
    "Last time we answered this question with hypothesis testing.  Now we will answer it with a classifier!\n",
    "\n",
    "### Recall Set Up\n",
    "\n",
    "In order to test this question, we can set up an experiment.  Here we will set up a randomized test group and a randomized control group.  \n",
    "\n",
    "The test group will be sent an email, with slightly different copy, or possibly with a picture.  Some specific change will be made, in any event.\n",
    "\n",
    "The control group will get the same email as last time.  This way, we can directly compare, as much as possible between the old email and the new one.  There are many things you typically need to control for, or account for in experimental design.  Some things to account for in this scenario are:\n",
    "\n",
    "1) Age\n",
    "\n",
    "2) Gender\n",
    "\n",
    "3) Location\n",
    "\n",
    "4) Time of Day\n",
    "\n",
    "5) Time of Year\n",
    "\n",
    "6) Approximate Disposable Income\n",
    "\n",
    "\n",
    "## Simulating Some Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    2822\n",
      "0    2178\n",
      "Name: converted, dtype: int64\n",
      "0    4393\n",
      "1     607\n",
      "Name: converted, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def generate_data(df, column, choices, size):\n",
    "    \"\"\"\n",
    "    Generates categorical data given choices.\n",
    "    \n",
    "    Parameters:\n",
    "    * df - pd.DataFrame: the data to add a column to\n",
    "    * column - str: the column to generate\n",
    "    * choices - list: the list of possible choices\n",
    "    \n",
    "    Returns:\n",
    "    A dataframe with the newly generated column.\n",
    "    \"\"\"\n",
    "    df[column] = [random.choice(choices)\n",
    "                  for _ in range(size)]\n",
    "    df = pd.concat([df, pd.get_dummies(df[column])], axis=1)\n",
    "    df = df.drop(column, axis=1)\n",
    "    return df\n",
    "\n",
    "def converted_score(x):\n",
    "    if x[\"male\"] == 1:\n",
    "        gender = 0.7\n",
    "    elif x[\"female\"] == 1:\n",
    "        gender = 1.4\n",
    "    if x[\"white\"] == 1:\n",
    "        race = 0.5\n",
    "    elif x[\"black\"] == 1:\n",
    "        race = 1.4\n",
    "    elif x[\"asian\"] == 1:\n",
    "        race = 2.8\n",
    "    elif x[\"hispanic\"] == 1:\n",
    "        race = 3.7\n",
    "    salary_alpha = gender * race\n",
    "    age_alpha = gender + race\n",
    "    return salary_alpha * x[\"salary\"] + age_alpha * x[\"age\"]\n",
    "\n",
    "def decision_boundary(result):\n",
    "    if result > 250000:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "size = 5000\n",
    "test_df = pd.DataFrame()\n",
    "control_df = pd.DataFrame()\n",
    "gender_choices = [\"male\", \"female\"]\n",
    "race_choices = [\"white\", \"black\", \"asian\", \"hispanic\"]\n",
    "test_salary_mean = 150000\n",
    "test_salary_variance = 30000 \n",
    "control_salary_mean = 55000\n",
    "control_salary_variance = 2000\n",
    "\n",
    "test_df = generate_data(test_df, \"gender\", gender_choices, size)\n",
    "test_df = generate_data(test_df, \"race\", race_choices, size)\n",
    "test_df[\"age\"] = np.random.normal(50, 25, size=len(test_df))\n",
    "test_df[\"age\"] = test_df[\"age\"].astype(int)\n",
    "test_df[\"salary\"] = np.random.normal(test_salary_mean, \n",
    "                                     test_salary_variance, \n",
    "                                     size=len(test_df))\n",
    "test_df[\"salary\"] = test_df[\"salary\"].apply(lambda x: round(x, 2))\n",
    "\n",
    "test_df[\"converted\"] = test_df.apply(converted_score, axis=1)\n",
    "test_df[\"converted\"] = test_df[\"converted\"].apply(decision_boundary)\n",
    "\n",
    "control_df = generate_data(control_df, \"gender\", gender_choices, size)\n",
    "control_df = generate_data(control_df, \"race\", race_choices, size)\n",
    "control_df[\"age\"] = np.random.normal(50, 25, size=len(control_df))\n",
    "control_df[\"age\"] = control_df[\"age\"].astype(int)\n",
    "control_df[\"salary\"] = np.random.normal(control_salary_mean, \n",
    "                                        control_salary_variance, \n",
    "                                        size=len(control_df))\n",
    "control_df[\"salary\"] = control_df[\"salary\"].apply(lambda x: round(x, 2))\n",
    "control_df[\"converted\"] = control_df.apply(converted_score, axis=1)\n",
    "control_df[\"converted\"] = control_df[\"converted\"].apply(decision_boundary)\n",
    "\n",
    "print(test_df[\"converted\"].value_counts())\n",
    "print(control_df[\"converted\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's where things get different! The next step is now to model our test and control and see if the probability of conversion is higher or lower for our test and control sets.  If they are the same or similar then our change likely had little effect.  Of course you should verify this with multiple tests as well as cross validation if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97       562\n",
      "           1       0.96      1.00      0.98       688\n",
      "\n",
      "    accuracy                           0.98      1250\n",
      "   macro avg       0.98      0.97      0.98      1250\n",
      "weighted avg       0.98      0.98      0.98      1250\n",
      "\n",
      "ROC AUC 0.9733096085409252\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "augmented_df = test_df.copy()\n",
    "augmented_y = augmented_df[\"converted\"]\n",
    "cols = augmented_df.columns.tolist()\n",
    "cols.remove(\"converted\")\n",
    "augmented_X = augmented_df[cols]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    augmented_X, augmented_y\n",
    ")\n",
    "test_clf = LogisticRegression(\n",
    "    max_iter=1000, class_weight=\"balanced\", \n",
    "    C=100, penalty=\"l2\", solver=\"liblinear\"\n",
    ")\n",
    "test_clf.fit(X_train, y_train)\n",
    "y_pred = test_clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC\", roc_auc_score(y_test, y_pred))\n",
    "plt.hist(clf.predict_proba(X_test).T[1])\n",
    "plt.title('Did Convert Probability Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1108\n",
      "           1       1.00      1.00      1.00       142\n",
      "\n",
      "    accuracy                           1.00      1250\n",
      "   macro avg       1.00      1.00      1.00      1250\n",
      "weighted avg       1.00      1.00      1.00      1250\n",
      "\n",
      "ROC AUC 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "control_y = control_df[\"converted\"]\n",
    "cols = control_df.columns.tolist()\n",
    "cols.remove(\"converted\")\n",
    "control_X = control_df[cols]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    control_X, control_y\n",
    ")\n",
    "control_clf = LogisticRegression(\n",
    "    max_iter=1000, class_weight=\"balanced\", \n",
    "    C=100, penalty=\"l2\", solver=\"liblinear\"\n",
    ")\n",
    "control_clf.fit(X_train, y_train)\n",
    "y_pred = control_clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC\", roc_auc_score(y_test, y_pred))\n",
    "plt.hist(clf.predict_proba(X_test).T[1])\n",
    "plt.title('Did Convert Probability Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "So we record all of the standard metrics to make sure our model explains what we are seeing well.  Then we look at the probability distributions for convert (y=1) and not convert (y=0).  If\n",
    "\n",
    "$$ P(converted | y=1) \\nsim P(converted | y=0)$$ \n",
    "\n",
    "then we can say our trial produced a meaningful result and we can say that our test data indeed changed something.\n",
    "\n",
    "Note: this is a worked example!  If this was the real world and you saw those accuracy measures, you ought to be very, very skeptical.  Especially the second set.\n",
    "\n",
    "Let's look at the specifics of our example:\n",
    "\n",
    "For test:\n",
    "\n",
    "* about 500 converted with a high probability and 450 converted with a very low probability.\n",
    "\n",
    "For control:\n",
    "\n",
    "* about 1200 people did not convert with a high probability and around 200 converted with a high probability.\n",
    "\n",
    "Clearly the test group had a different experience!  But why?  Let's dig further into the data by looking at `test_df` and `control_df`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>female</th>\n",
       "      <th>male</th>\n",
       "      <th>asian</th>\n",
       "      <th>black</th>\n",
       "      <th>hispanic</th>\n",
       "      <th>white</th>\n",
       "      <th>age</th>\n",
       "      <th>salary</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.503600</td>\n",
       "      <td>0.496400</td>\n",
       "      <td>0.250800</td>\n",
       "      <td>0.258800</td>\n",
       "      <td>0.247800</td>\n",
       "      <td>0.242600</td>\n",
       "      <td>50.339400</td>\n",
       "      <td>150040.746312</td>\n",
       "      <td>0.564400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.500037</td>\n",
       "      <td>0.500037</td>\n",
       "      <td>0.433517</td>\n",
       "      <td>0.438019</td>\n",
       "      <td>0.431778</td>\n",
       "      <td>0.428698</td>\n",
       "      <td>24.645724</td>\n",
       "      <td>30019.400205</td>\n",
       "      <td>0.495885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-37.000000</td>\n",
       "      <td>41819.190000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>130438.922500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>150082.165000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>170778.290000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>256016.380000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            female         male        asian        black     hispanic  \\\n",
       "count  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000   \n",
       "mean      0.503600     0.496400     0.250800     0.258800     0.247800   \n",
       "std       0.500037     0.500037     0.433517     0.438019     0.431778   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       1.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       1.000000     1.000000     1.000000     1.000000     0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "             white          age         salary    converted  \n",
       "count  5000.000000  5000.000000    5000.000000  5000.000000  \n",
       "mean      0.242600    50.339400  150040.746312     0.564400  \n",
       "std       0.428698    24.645724   30019.400205     0.495885  \n",
       "min       0.000000   -37.000000   41819.190000     0.000000  \n",
       "25%       0.000000    34.000000  130438.922500     0.000000  \n",
       "50%       0.000000    50.000000  150082.165000     1.000000  \n",
       "75%       0.000000    67.000000  170778.290000     1.000000  \n",
       "max       1.000000   143.000000  256016.380000     1.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>female</th>\n",
       "      <th>male</th>\n",
       "      <th>asian</th>\n",
       "      <th>black</th>\n",
       "      <th>hispanic</th>\n",
       "      <th>white</th>\n",
       "      <th>age</th>\n",
       "      <th>salary</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.504400</td>\n",
       "      <td>0.495600</td>\n",
       "      <td>0.264600</td>\n",
       "      <td>0.243800</td>\n",
       "      <td>0.243200</td>\n",
       "      <td>0.248400</td>\n",
       "      <td>49.159800</td>\n",
       "      <td>54987.455054</td>\n",
       "      <td>0.121400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.500031</td>\n",
       "      <td>0.500031</td>\n",
       "      <td>0.441164</td>\n",
       "      <td>0.429416</td>\n",
       "      <td>0.429058</td>\n",
       "      <td>0.432128</td>\n",
       "      <td>24.594407</td>\n",
       "      <td>1986.156938</td>\n",
       "      <td>0.326624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-40.000000</td>\n",
       "      <td>48168.270000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>53631.270000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>55009.735000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>56297.607500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>63514.950000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            female         male        asian        black     hispanic  \\\n",
       "count  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000   \n",
       "mean      0.504400     0.495600     0.264600     0.243800     0.243200   \n",
       "std       0.500031     0.500031     0.441164     0.429416     0.429058   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       1.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       1.000000     1.000000     1.000000     0.000000     0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "             white          age        salary    converted  \n",
       "count  5000.000000  5000.000000   5000.000000  5000.000000  \n",
       "mean      0.248400    49.159800  54987.455054     0.121400  \n",
       "std       0.432128    24.594407   1986.156938     0.326624  \n",
       "min       0.000000   -40.000000  48168.270000     0.000000  \n",
       "25%       0.000000    32.000000  53631.270000     0.000000  \n",
       "50%       0.000000    49.000000  55009.735000     0.000000  \n",
       "75%       0.000000    66.000000  56297.607500     0.000000  \n",
       "max       1.000000   145.000000  63514.950000     1.000000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a huge difference in salary!  Let's look at how big the difference is on average:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95053.29125800001"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[\"salary\"].mean() - control_df[\"salary\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Around 95K!  That's a huge difference in standard of living.  Let's go back to our model and look at our coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "female 0.9292687935837641\n",
      "male -4.103078995398313\n",
      "asian 2.6485300549834054\n",
      "black -2.6993728240235115\n",
      "hispanic 4.453509716475326\n",
      "white -7.576477149249771\n",
      "age -0.0009181427593822856\n",
      "salary 4.08325526503341e-05\n"
     ]
    }
   ],
   "source": [
    "for index, coef in enumerate(test_clf.coef_[0]):\n",
    "    print(test_df.columns[index], coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "female 6.9140512773516924\n",
      "male -7.395307616549721\n",
      "asian -3.8570837201125263\n",
      "black -3.806284792620383\n",
      "hispanic 11.029417213298009\n",
      "white -3.847305039763138\n",
      "age -0.0029246918380587596\n",
      "salary -0.00018044270948364995\n"
     ]
    }
   ],
   "source": [
    "for index, coef in enumerate(control_clf.coef_[0]):\n",
    "    print(control_df.columns[index], coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The big thing to pay attention to here is effect size - how the size of the variable and it's weight changes the decision or result of the model.\n",
    "\n",
    "The effect size of salary may _seem_ small based on these coefficients, but I assure you it's not.  Recall, that for logistic regression the equation is:\n",
    "\n",
    "$$ \\theta_{i}x_{i} $$\n",
    "\n",
    "That means we need to consider the _multiplication_ of these two variables.  Let's just look at the average salary for the two groups to get a sense of effect size:\n",
    "\n",
    "For test:\n",
    "\n",
    "$$ \\theta * mean(salary) = 6.126546673480162 $$\n",
    "\n",
    "That is, `4.08325526503341e-05 * 150040.746312 = 6.126546673480162`.\n",
    "\n",
    "Given that effect size is exponentiated by the base, in this case $e$, the natural number.  We have, an increase in salary by 1 dollar leading to an increase in P(Y=1) of:\n",
    "\n",
    "$$ e^{6.126546673480162} = 457.85231394605876 $$\n",
    "\n",
    "Which is _a lot_.  Especially given the effect size of the other coefficients.\n",
    "\n",
    "I'll leave as an exercise carrying this out for the other model.  But let's just say the effect size is similar.\n",
    "\n",
    "What that implies should be obvious, how much you make matters a ton!  So can we tell if the change actually meant anything?  Nope!  We have no idea.  We should construct this problem again, controlling explicitly for income in order to get a fair test.\n",
    "\n",
    "## Example Two - Customer Churn\n",
    "\n",
    "The next typical data science problem we are going to tackle is customer churn!  This is huge for product development, sales cycles and just keeping a business afloat.  If you know and can predict how much your customers are going to churn you can reliably forecast how much revenue to expect per quarter.  Which is basically essential to any and all businesses.  \n",
    "\n",
    "Since customer churn is so important, it's worth noting that there is more to churn than just what's in your model.  It can help drive decisions, but it's very important to include domain experts in churn conversations.  This means designers, UX and design folks, sales folks, executives and other stakeholders.  All of these folks matter.  For one, your model may not take into account critical variables.  For another thing, you may not be measuring enough.  A good model doesn't replace people, it helps inform a conversation and aids in decision making.\n",
    "\n",
    "Let's start with some context for what Customer Churn is and go through some possible definitions:\n",
    "\n",
    "Customer churn, loosely, is the number of customers that will stop paying for your service over a specified period of time.\n",
    "\n",
    "Definition one:\n",
    "\n",
    "$$ \\frac{customers \\space lost \\space during \\space fixed \\space period}{total \\space customers \\space at \\space the \\space start \\space of \\space the \\space fixed \\space period} $$\n",
    "\n",
    "Notice, this does not take into account the total number of customers gained.  Say for instance you started out with 100 customers, then you gained 1 million over the course of the fixed period, say a month, and then 15 thousand churned.  Your churn for the month would be:\n",
    "\n",
    "$$ \\frac{15000}{100} = 150 \\% $$\n",
    "\n",
    "If 150% of your customers churn, you might think your business is _not_ doing great.  But you gained like a million new customers over the period!  So in actuality, this is very good news, _overall_.  If you don't get a ton of new customers, this might be a good enough metric.\n",
    "\n",
    "Definition two:\n",
    "\n",
    "$$ \\frac{Churn}{Customers_{1} + Customers_{n} / 2} $$\n",
    "\n",
    "Where:\n",
    "\n",
    "$ Customer_{1} $ := number of customers at the start of the month\n",
    "\n",
    "$ Customer_{n} $ := number of customers at the end of the month\n",
    "\n",
    "So over the same window we get:\n",
    "\n",
    "$$ \\frac{15000}{(100 + 1000000)/2} = 0.029\\% $$\n",
    "\n",
    "Which sounds much more reasonable, and accurate.\n",
    "\n",
    "Definition three:\n",
    "\n",
    "$$ \\frac{Churn}{\\frac{1}{n} \\displaystyle \\sum_{i=1}^{n} Customer_{i}} $$\n",
    "\n",
    "Here we take the average of the customer count over the period of interest, this further normalizes the churn.\n",
    "\n",
    "## Modeling Churn\n",
    "\n",
    "Once you have a good measure of Churn, that works for you, the next step is to understand your Churn number.  For this we'll create a model of the world that incorpates other data to understand when and more importantly why customers churn.\n",
    "\n",
    "For this example we will be making use of this dataset from kaggle:\n",
    "\n",
    "https://www.kaggle.com/adammaus/predicting-churn-for-bank-customers\n",
    "\n",
    "To get this part of the notebook to run, you'll need to download and unzip the data locally (a pain I know).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 12 columns):\n",
      "Surname            10000 non-null object\n",
      "CreditScore        10000 non-null int64\n",
      "Geography          10000 non-null object\n",
      "Gender             10000 non-null object\n",
      "Age                10000 non-null int64\n",
      "Tenure             10000 non-null int64\n",
      "Balance            10000 non-null float64\n",
      "NumOfProducts      10000 non-null int64\n",
      "HasCrCard          10000 non-null int64\n",
      "IsActiveMember     10000 non-null int64\n",
      "EstimatedSalary    10000 non-null float64\n",
      "Churn              10000 non-null int64\n",
      "dtypes: float64(2), int64(7), object(3)\n",
      "memory usage: 937.6+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"Churn_Modelling.csv\")\n",
    "df.drop(\"RowNumber\", inplace=True, axis=1)\n",
    "df.drop(\"CustomerId\", inplace=True, axis=1)\n",
    "df[\"Churn\"] = df[\"Exited\"] \n",
    "df.drop(\"Exited\", inplace=True, axis=1)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop the `RowNumber` and the `CustomerId` since they won't be useful for understanding why our customers churn or stay.  Some of the other information may be useful.  First let's look at `Gender`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Churn %</th>\n",
       "      <th>Churn Total</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gender</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Female</th>\n",
       "      <td>0.250715</td>\n",
       "      <td>1139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Male</th>\n",
       "      <td>0.164559</td>\n",
       "      <td>898</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Churn %  Churn Total\n",
       "Gender                       \n",
       "Female  0.250715         1139\n",
       "Male    0.164559          898"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = df.pivot_table(values=\"Churn\", index='Gender', aggfunc=np.mean)\n",
    "summary[\"Churn %\"] = summary[\"Churn\"] \n",
    "summary[\"Churn Total\"] = df.pivot_table(values=\"Churn\", index=\"Gender\", aggfunc=np.sum)\n",
    "summary.drop(\"Churn\", axis=1, inplace=True)\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems pretty clear that `Gender` is going to matter, and that `Churn` is going to be higher for women than men.\n",
    "\n",
    "Now let's see if this is True for all countries under consideration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['France' 'Spain' 'Germany']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Geography</th>\n",
       "      <th>France</th>\n",
       "      <th>Germany</th>\n",
       "      <th>Spain</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gender</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Female</th>\n",
       "      <td>0.203450</td>\n",
       "      <td>0.375524</td>\n",
       "      <td>0.212121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Male</th>\n",
       "      <td>0.127134</td>\n",
       "      <td>0.278116</td>\n",
       "      <td>0.131124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Geography    France   Germany     Spain\n",
       "Gender                                 \n",
       "Female     0.203450  0.375524  0.212121\n",
       "Male       0.127134  0.278116  0.131124"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df[\"Geography\"].unique())\n",
    "summary = df.pivot_table(values=\"Churn\", index=['Gender'], columns=[\"Geography\"], aggfunc=np.mean)\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears churn rates are higher across the board for women over men, however churn rates do vary from country to country:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['France' 'Spain' 'Germany']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Geography</th>\n",
       "      <th>France</th>\n",
       "      <th>Germany</th>\n",
       "      <th>Spain</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gender</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Female</th>\n",
       "      <td>460</td>\n",
       "      <td>448</td>\n",
       "      <td>231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Male</th>\n",
       "      <td>350</td>\n",
       "      <td>366</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Geography  France  Germany  Spain\n",
       "Gender                           \n",
       "Female        460      448    231\n",
       "Male          350      366    182"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df[\"Geography\"].unique())\n",
    "summary = df.pivot_table(values=\"Churn\", index=['Gender'], columns=[\"Geography\"], aggfunc=np.sum)\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is probably a function of population size.  However this means explicitly controlling for `Geography` is probably important, because otherwise we may lose a confounding effect, which would lower the generalizability of our analysis.\n",
    "\n",
    "Next let's look at the effect of age on Churn.  For this we should first run the test for independence followed by a test for correlation.  Recall, we will use Kruskal-Wallis for independence and point bi serial correlation for correlation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KruskalResult(statistic=16030.329492796818, pvalue=0.0)\n",
      "PointbiserialrResult(correlation=0.28532303783506824, pvalue=1.2399313093495365e-186)\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "print(stats.kruskal(df[\"Age\"], df[\"Churn\"]))\n",
    "print(stats.pointbiserialr(df[\"Age\"], df[\"Churn\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a pvalue of zero we reject the null hypothesis that the two variables are independent.  Additionally, we see a pvalue of close to zero for point bi serial correlation, therefore we reject the null hypothesis of no correlation.  So `Age` is a variable of interest.  We can also confirm this with mutual information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05610117])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import feature_selection\n",
    "feature_selection.mutual_info_regression(df[\"Age\"].values.reshape(-1, 1), df[\"Churn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't read An Introduction to Information Theory yet, the important things to note are when mutual information is 1, the variables are perfectly dependent.  When it's zero they are the same.  So there is some weak information sharing between `Age` and `Churn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can actually go through the rest of the variables to see which ones are likely useful for predicting churn with mutual information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Age', 0.09048326288892028),\n",
       " ('NumOfProducts', 0.07303749592710229),\n",
       " ('IsActiveMember', 0.013130240845428354),\n",
       " ('Tenure', 0.00409941408302128),\n",
       " ('EstimatedSalary', 0.0027089277988778804),\n",
       " ('Balance', 0.00161677556992057),\n",
       " ('CreditScore', 0.0006704402476174209),\n",
       " ('HasCrCard', 0.0)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = [column for column in df.columns if df[column].dtype != \"object\"]\n",
    "features.remove(\"Churn\")\n",
    "X = df[features]\n",
    "y = df[\"Churn\"]\n",
    "ranks = feature_selection.mutual_info_regression(X, y)\n",
    "rankings = []\n",
    "for index, feature in enumerate(features):\n",
    "    rankings.append((feature, ranks[index]))\n",
    "sorted(rankings, key=lambda t: t[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that `NumOfProducts` is the most important and `HasCrCard` is the least important.  Or rather carries the least information about probability of churning.  This is possibly because most people have a credit card these days.\n",
    "\n",
    "Next let's kick out any variables that don't likely matter.  In this case, we can probably safely remove `HasCrCard`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"HasCrCard\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also drop `Surname`, since any information it gives us is probably spurious:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"Surname\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least, let's look at how balanced our classes are.  This will inform what type of classifier we use and what kind of accuracy we can expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7963\n",
       "1    2037\n",
       "Name: Churn, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Churn\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there is some imbalance with about 80% of people not churning from the service.  So we'll need to explicitly account for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are almost ready to fit our model!  Let's first get our categorical variables ready:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    if df[column].dtype == \"object\":\n",
    "        df = pd.concat([df, pd.get_dummies(df[column])], axis=1)\n",
    "        df = df.drop(column, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.65      0.74      1944\n",
      "           1       0.35      0.66      0.46       556\n",
      "\n",
      "    accuracy                           0.65      2500\n",
      "   macro avg       0.61      0.66      0.60      2500\n",
      "weighted avg       0.76      0.65      0.68      2500\n",
      "\n",
      "0.6562953341030878\n",
      "11.978264824417066\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score, log_loss\n",
    "\n",
    "y = df[\"Churn\"]\n",
    "cols = df.columns.tolist()\n",
    "cols.remove(\"Churn\")\n",
    "X = df[cols]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=12)\n",
    "logit = LogisticRegression(\n",
    "    C=1, penalty=\"l2\", max_iter=1000, \n",
    "    class_weight=\"balanced\"\n",
    ")\n",
    "logit.fit(X_train, y_train)\n",
    "y_pred = logit.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(roc_auc_score(y_test, y_pred))\n",
    "print(log_loss(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One strategy to deal with class imbalance is by making that explicit.  Here we the `class_weight` hyperparameter to try and adjust for this class imbalance.  What this does is weight examples from the minority class higher than those from the majority class.  Unfortunately our model doesn't do great, so let's see if there are any obvious variables we can remove:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CreditScore -0.0031436478957918785\n",
      "Age 0.05898746159233117\n",
      "Tenure -0.08679665322930301\n",
      "Balance 3.905155260992167e-06\n",
      "NumOfProducts -0.039819154004494525\n",
      "IsActiveMember -0.07847513122067634\n",
      "EstimatedSalary -9.223377167585139e-07\n",
      "France -0.04440259570810289\n",
      "Germany 0.05012964004326264\n",
      "Spain -0.022347366299728076\n",
      "Female 0.04293418313482472\n",
      "Male -0.059554505098413556\n"
     ]
    }
   ],
   "source": [
    "for index, col in enumerate(cols):\n",
    "    print(col, logit.coef_[0][index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of the variables really jump out as the issue.  You might think that `EstimatedSalary` and `Balance` are the issue, but both of them have much larger values than the rest of the variables.  Let's just confirm that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>650.528800</td>\n",
       "      <td>38.921800</td>\n",
       "      <td>5.012800</td>\n",
       "      <td>76485.889288</td>\n",
       "      <td>1.530200</td>\n",
       "      <td>0.515100</td>\n",
       "      <td>100090.239881</td>\n",
       "      <td>0.203700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>96.653299</td>\n",
       "      <td>10.487806</td>\n",
       "      <td>2.892174</td>\n",
       "      <td>62397.405202</td>\n",
       "      <td>0.581654</td>\n",
       "      <td>0.499797</td>\n",
       "      <td>57510.492818</td>\n",
       "      <td>0.402769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>350.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.580000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>584.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51002.110000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>652.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>97198.540000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100193.915000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>718.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>127644.240000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>149388.247500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>850.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>250898.090000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>199992.480000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        CreditScore           Age        Tenure        Balance  NumOfProducts  \\\n",
       "count  10000.000000  10000.000000  10000.000000   10000.000000   10000.000000   \n",
       "mean     650.528800     38.921800      5.012800   76485.889288       1.530200   \n",
       "std       96.653299     10.487806      2.892174   62397.405202       0.581654   \n",
       "min      350.000000     18.000000      0.000000       0.000000       1.000000   \n",
       "25%      584.000000     32.000000      3.000000       0.000000       1.000000   \n",
       "50%      652.000000     37.000000      5.000000   97198.540000       1.000000   \n",
       "75%      718.000000     44.000000      7.000000  127644.240000       2.000000   \n",
       "max      850.000000     92.000000     10.000000  250898.090000       4.000000   \n",
       "\n",
       "       IsActiveMember  EstimatedSalary         Churn  \n",
       "count    10000.000000     10000.000000  10000.000000  \n",
       "mean         0.515100    100090.239881      0.203700  \n",
       "std          0.499797     57510.492818      0.402769  \n",
       "min          0.000000        11.580000      0.000000  \n",
       "25%          0.000000     51002.110000      0.000000  \n",
       "50%          1.000000    100193.915000      0.000000  \n",
       "75%          1.000000    149388.247500      0.000000  \n",
       "max          1.000000    199992.480000      1.000000  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the averages for `EstimatedSalary` and `Balance` are much much higher than the other variables.  So even though they have smaller coefficients, their effect size will be large enough to contribute to the decision boundary.  Next, let's see if the solver or penalty is the issue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.72      0.80      1944\n",
      "           1       0.42      0.70      0.53       556\n",
      "\n",
      "    accuracy                           0.72      2500\n",
      "   macro avg       0.66      0.71      0.66      2500\n",
      "weighted avg       0.79      0.72      0.74      2500\n",
      "\n",
      "0.7127298161470823\n",
      "9.740107656409736\n"
     ]
    }
   ],
   "source": [
    "logit_linear = LogisticRegression(\n",
    "    solver=\"liblinear\",\n",
    "    C=1, penalty=\"l1\", max_iter=10000, \n",
    "    class_weight=\"balanced\"\n",
    ")\n",
    "logit_linear.fit(X_train, y_train)\n",
    "y_pred = logit_linear.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(roc_auc_score(y_test, y_pred))\n",
    "print(log_loss(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CreditScore -0.0006180222871635596\n",
      "Age 0.0782462365221455\n",
      "Tenure -0.006832107433490348\n",
      "Balance 2.6100726865106033e-06\n",
      "NumOfProducts -0.125957377561036\n",
      "IsActiveMember -0.8492739668684903\n",
      "EstimatedSalary 1.2544822574956195e-07\n",
      "France -0.807417662931865\n",
      "Germany 0.0\n",
      "Spain -0.8244401538543713\n",
      "Female -0.34300710576221705\n",
      "Male -0.9037524239656831\n"
     ]
    }
   ],
   "source": [
    "for index, col in enumerate(cols):\n",
    "    print(col, logit_linear.coef_[0][index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do a little bit better with this model, but still not well enough.  Time to bring in another library specifically created to deal with imbalanced classes.\n",
    "\n",
    "## Dealing with imbalanced data explicitly\n",
    "\n",
    "There are a few strategies for dealing with imbalanced data.  We've already looked at class weight balancing, which can work in some cases.  But unfortunately doesn't work for our example problem.  Next let's look at under and over sampling.  Before we go through that, let's define what sampling is and why it can be effective.\n",
    "\n",
    "### Sampling\n",
    "\n",
    "In a perfect world, we would never need to sample.  We would always have enough data and that data would be our population of interest.  We would know everything about it that was relevant and we would be able to accurately come up with policy interventions or understand some phenomenon.  Unfortunately, in the real world, we have no such luck.  We rarely, if ever, have all the data about our population of interest.  Does that mean that our models are not faithful to the real world, if they are trained on limited data?  The answer is not necessarily!\n",
    "\n",
    "In chapter one we introduced the notion of the distribution and descriptive statistics.  Loosely defined, a sample is a distribution of data that comes from a large population distribution.  One way to measure similarity between the sample and the population distribution is to see if their descriptive statistics, also known as their characteristics, are similar.\n",
    "\n",
    "Let's look at an example!\n",
    "\n",
    "Suppose there exists the following population distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAE9JJREFUeJzt3X+s3fV93/HnK5SQaulmU26Za5vZalxVpFsBecCU/cFggKFVTaUuIpoSlyG5nUBKpGiNSaXRJkUiWhs21BSNFjdmyupa+SGsxB11CFOVPwAbQiCGMO5CGLYMuDUhidCYTN7743yAE3IvPvfec++5vp/nQzq63/P+fr/nfr5H9nndz+f7+X5PqgpJUn/eMekGSJImwwCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdeqnJt2At3PWWWfVhg0bJt0MSTqlPPzww39XVVMn225ZB8CGDRs4ePDgpJshSaeUJM+Osp1DQJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1KllfSWwtJxt2PGVN5a/e+uvznsbaVLsAUhSp+wBSGPgX/o6FdkDkKRO2QOQxmy4NyAtZ/YAJKlTBoAkdcohIGmJzDY05EljTYoBIM2B4/taSRwCkqROnTQAkrwryUNJvpnkUJI/aPXPJnkmyaPtcV6rJ8ntSaaTPJbkgqHX2pbk6fbYtniHJZ06Nuz4yhsPaSmNMgT0KnBpVf0wyenA15P8dVv3H6rq82/Z/ipgU3tcBNwBXJTkTOBmYDNQwMNJ9lbVS+M4EEnS3Jy0B1ADP2xPT2+PeptdtgJ3t/0eAFYlWQNcCeyvquPtQ38/sGVhzZckzddIJ4GTnAY8DLwH+ExVPZjk3wO3JPmPwH3Ajqp6FVgLPDe0++FWm60uLWsOzWilGukkcFW9VlXnAeuAC5P8MnAT8EvAPwfOBD42jgYl2Z7kYJKDx44dG8dLSpJmMKdZQFX1PeB+YEtVHW3DPK8CfwFc2DY7Aqwf2m1dq81Wf+vvuLOqNlfV5qmpqbk0T5I0B6PMAppKsqot/zRwOfDtNq5PkgDXAN9qu+wFPtRmA10MvFxVR4F7gSuSrE6yGrii1SRJEzDKOYA1wK52HuAdwJ6q+nKSryWZAgI8CvxO234fcDUwDbwCXAdQVceTfBI40Lb7RFUdH9+hSKc+byutpXTSAKiqx4DzZ6hfOsv2Bdwwy7qdwM45tlGStAi8EliSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1yi+EkWbg/X/UAwNAavzQV28cApKkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVNeByAtU345jBabPQBJ6pQBIEmdcghIXfP2D+rZSXsASd6V5KEk30xyKMkftPrGJA8mmU7yV0ne2epntOfTbf2Gode6qdWfSnLlYh2UJOnkRhkCehW4tKp+BTgP2JLkYuBTwG1V9R7gJeD6tv31wEutflvbjiTnAtcC7wW2AH+a5LRxHowkaXQnDYAa+GF7enp7FHAp8PlW3wVc05a3tue09ZclSavvrqpXq+oZYBq4cCxHIa1wG3Z85Y2HNC4jnQROclqSR4EXgf3A/wa+V1Un2iaHgbVteS3wHEBb/zLws8P1GfYZ/l3bkxxMcvDYsWNzPyJJ0khGCoCqeq2qzgPWMfir/ZcWq0FVdWdVba6qzVNTU4v1aySpe3OaBlpV3wPuB/4FsCrJ67OI1gFH2vIRYD1AW/+PgL8frs+wjyRpiY0yC2gqyaq2/NPA5cCTDILgN9tm24B72vLe9py2/mtVVa1+bZsltBHYBDw0rgORJM3NKNcBrAF2tRk77wD2VNWXkzwB7E7yh8A3gLva9ncB/y3JNHCcwcwfqupQkj3AE8AJ4Iaqem28hyNJGtVJA6CqHgPOn6H+HWaYxVNV/xf4N7O81i3ALXNvpiRp3LwVhCR1ygCQpE55LyDpFONtojUu9gAkqVMGgCR1ygCQpE4ZAJLUKQNAkjrlLCB1x1sqSwP2ACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnnAaqLqzUqZ/eGE4LYQ9AkjplAEhSp0b5Uvj1Se5P8kSSQ0k+3Oq/n+RIkkfb4+qhfW5KMp3kqSRXDtW3tNp0kh2Lc0iSpFGMcg7gBPDRqnokyc8ADyfZ39bdVlV/NLxxknMZfBH8e4GfB76a5Bfb6s8AlwOHgQNJ9lbVE+M4EEnS3IzypfBHgaNt+QdJngTWvs0uW4HdVfUq8EySad788vjp9mXyJNndtjUAJGkC5nQOIMkG4HzgwVa6McljSXYmWd1qa4HnhnY73Gqz1SVJEzByACR5N/AF4CNV9X3gDuAXgPMY9BD+eBwNSrI9ycEkB48dOzaOl5QkzWCkAEhyOoMP/89V1RcBquqFqnqtqn4E/BlvDvMcAdYP7b6u1War/5iqurOqNlfV5qmpqbkejyRpRKPMAgpwF/BkVX16qL5maLPfAL7VlvcC1yY5I8lGYBPwEHAA2JRkY5J3MjhRvHc8hyFJmqtRZgG9D/gg8HiSR1vt48AHkpwHFPBd4LcBqupQkj0MTu6eAG6oqtcAktwI3AucBuysqkNjPBZJ0hyMMgvo60BmWLXvbfa5Bbhlhvq+t9tPkrR0vBeQVqyVev+f2XhfIM2Vt4KQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1ClvBSGtQN4WQqMwALRi9HbvH2mhHAKSpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTppACRZn+T+JE8kOZTkw61+ZpL9SZ5uP1e3epLcnmQ6yWNJLhh6rW1t+6eTbFu8w5Ikncwo1wGcAD5aVY8k+Rng4ST7gd8C7quqW5PsAHYAHwOuAja1x0XAHcBFSc4EbgY2A9VeZ29VvTTug5L0prdeH+GFYXrdSXsAVXW0qh5pyz8AngTWAluBXW2zXcA1bXkrcHcNPACsSrIGuBLYX1XH24f+fmDLWI9GkjSyOV0JnGQDcD7wIHB2VR1tq54Hzm7La4HnhnY73Gqz1aV58+pfaf5GPgmc5N3AF4CPVNX3h9dVVTEY1lmwJNuTHExy8NixY+N4SUnSDEYKgCSnM/jw/1xVfbGVX2hDO7SfL7b6EWD90O7rWm22+o+pqjuranNVbZ6amprLsUiS5mCUWUAB7gKerKpPD63aC7w+k2cbcM9Q/UNtNtDFwMttqOhe4Iokq9uMoStaTZI0AaOcA3gf8EHg8SSPttrHgVuBPUmuB54F3t/W7QOuBqaBV4DrAKrqeJJPAgfadp+oquNjOQpJ0pydNACq6utAZll92QzbF3DDLK+1E9g5lwZKkhaHVwJLUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkTs3pXkDScuD9f6TxMACkzgwHqLeG7ptDQJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcrrAKSOeU1A3+wBSFKnDABJ6tQoXwq/M8mLSb41VPv9JEeSPNoeVw+tuynJdJKnklw5VN/SatNJdoz/UCRJczHKOYDPAn8C3P2W+m1V9UfDhSTnAtcC7wV+Hvhqkl9sqz8DXA4cBg4k2VtVTyyg7eqIN4CTxm+UL4X/2yQbRny9rcDuqnoVeCbJNHBhWzddVd8BSLK7bWsASNKELOQcwI1JHmtDRKtbbS3w3NA2h1tttrokaULmGwB3AL8AnAccBf54XA1Ksj3JwSQHjx07Nq6XlSS9xbwCoKpeqKrXqupHwJ/x5jDPEWD90KbrWm22+kyvfWdVba6qzVNTU/NpniRpBPMKgCRrhp7+BvD6DKG9wLVJzkiyEdgEPAQcADYl2ZjknQxOFO+df7MlSQt10pPASf4SuAQ4K8lh4GbgkiTnAQV8F/htgKo6lGQPg5O7J4Abquq19jo3AvcCpwE7q+rQ2I9GkjSyUWYBfWCG8l1vs/0twC0z1PcB++bUOnXNqZ/S4vJKYEnqlAEgSZ0yACSpU94OWhLgraF7ZA9AkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI65ZXAWla8A6i0dOwBSFKn7AFI+gneF6gP9gAkqVMGgCR1yiEgSW/L4aCV66Q9gCQ7k7yY5FtDtTOT7E/ydPu5utWT5PYk00keS3LB0D7b2vZPJ9m2OIcjSRrVKENAnwW2vKW2A7ivqjYB97XnAFcBm9pjO3AHDAIDuBm4CLgQuPn10JAkTcZJA6Cq/hY4/pbyVmBXW94FXDNUv7sGHgBWJVkDXAnsr6rjVfUSsJ+fDBVJ0hKa70ngs6vqaFt+Hji7La8Fnhva7nCrzVaXJE3Igk8CV1UlqXE0BiDJdgbDR5xzzjnjelktY179K03GfHsAL7ShHdrPF1v9CLB+aLt1rTZb/SdU1Z1VtbmqNk9NTc2zeZKkk5lvAOwFXp/Jsw24Z6j+oTYb6GLg5TZUdC9wRZLV7eTvFa0mSZqQkw4BJflL4BLgrCSHGczmuRXYk+R64Fng/W3zfcDVwDTwCnAdQFUdT/JJ4EDb7hNV9dYTy5KkJXTSAKiqD8yy6rIZti3ghlleZyewc06tkyQtGm8FIUmd8lYQmghn/kiTZw9AkjplD0DSyLwx3MpiD0CSOmUASFKnHAKSNC8OB5367AFIUqcMAEnqlAEgSZ3yHICWjBd/ScuLPQBJ6pQBIEmdcghI0oI5JfTUZABoUTnuLy1fDgFJUqcMAEnqlAEgSZ0yACSpUwsKgCTfTfJ4kkeTHGy1M5PsT/J0+7m61ZPk9iTTSR5LcsE4DkCSND/j6AH8q6o6r6o2t+c7gPuqahNwX3sOcBWwqT22A3eM4XdLkuZpMaaBbgUuacu7gP8JfKzV766qAh5IsirJmqo6ughtkDQhXhNw6lhoABTwN0kK+K9VdSdw9tCH+vPA2W15LfDc0L6HW+3HAiDJdgY9BM4555wFNk+T4Nx/6dSw0AD4l1V1JMnPAfuTfHt4ZVVVC4eRtRC5E2Dz5s1z2leSNLoFBUBVHWk/X0zyJeBC4IXXh3aSrAFebJsfAdYP7b6u1SStUA4HLW/zDoAk/wB4R1X9oC1fAXwC2AtsA25tP+9pu+wFbkyyG7gIeNnx/5XDYR/p1LOQHsDZwJeSvP46/72q/keSA8CeJNcDzwLvb9vvA64GpoFXgOsW8LslSQs07wCoqu8AvzJD/e+By2aoF3DDfH+fJGm8vBJYkjplAEhSp/w+AElLwhlBy48BoHlz5o90ajMAJC05ewPLg+cAJKlT9gA0Jw77aNzsDUyOPQBJ6pQ9AJ2Uf/VLK5M9AEnqlAEgSZ1yCEgzcthHWvkMAEnLhjOClpYBIGlZMgwWn+cAJKlT9gD0Bsf9tVzZG1gcBkCH/KDXqcwwGB+HgCSpU0veA0iyBfgvwGnAn1fVrUvdhh75V79WInsDC7OkAZDkNOAzwOXAYeBAkr1V9cRStqMXfuirJ4bB3C11D+BCYLp9oTxJdgNbAQNgTPzQl2b/f2Aw/LilDoC1wHNDzw8DFy1xG5aF2f5a8QNcWjzz+f+1kkNj2c0CSrId2N6e/jDJU5NszwKcBfzdKBvmU4vckskY+fhXsN7fgxVx/Av8/zmp9+CfjLLRUgfAEWD90PN1rfaGqroTuHMpG7UYkhysqs2Tbsek9H784HvQ+/HD8n8Plnoa6AFgU5KNSd4JXAvsXeI2SJJY4h5AVZ1IciNwL4NpoDur6tBStkGSNLDk5wCqah+wb6l/7wSc8sNYC9T78YPvQe/HD8v8PUhVTboNkqQJ8FYQktQpA2ARJPlokkpyVnueJLcnmU7yWJILJt3GxZLkPyX5djvOLyVZNbTupvYePJXkykm2czEl2dKOcTrJjkm3ZykkWZ/k/iRPJDmU5MOtfmaS/Umebj9XT7qtiynJaUm+keTL7fnGJA+2fwt/1Sa/LBsGwJglWQ9cAfyfofJVwKb22A7cMYGmLZX9wC9X1T8D/hdwE0CScxnM+novsAX403ZrkBVl6HYnVwHnAh9ox77SnQA+WlXnAhcDN7Tj3gHcV1WbgPva85Xsw8CTQ88/BdxWVe8BXgKun0irZmEAjN9twO8CwydXtgJ318ADwKokaybSukVWVX9TVSfa0wcYXOsBg/dgd1W9WlXPANMMbg2y0rxxu5Oq+n/A67c7WdGq6mhVPdKWf8DgQ3Atg2Pf1TbbBVwzmRYuviTrgF8F/rw9D3Ap8Pm2ybI7fgNgjJJsBY5U1TffsmqmW2CsXbKGTc6/A/66LffyHvRynLNKsgE4H3gQOLuqjrZVzwNnT6hZS+E/M/jj70ft+c8C3xv6g2jZ/VtYdreCWO6SfBX4xzOs+j3g4wyGf1a0t3sPquqets3vMRgW+NxStk2TleTdwBeAj1TV9wd/BA9UVSVZkdMOk/wa8GJVPZzkkkm3Z1QGwBxV1b+eqZ7knwIbgW+2f/TrgEeSXMgIt8A4lcz2HrwuyW8BvwZcVm/OM15R78Hb6OU4f0KS0xl8+H+uqr7Yyi8kWVNVR9uw54uTa+Gieh/w60muBt4F/EMG33uyKslPtV7Asvu34BDQmFTV41X1c1W1oao2MOjuXVBVzzO43cWH2mygi4GXh7rFK0r7wp/fBX69ql4ZWrUXuDbJGUk2Mjgh/tAk2rjIurzdSRvvvgt4sqo+PbRqL7CtLW8D7lnqti2Fqrqpqta1//vXAl+rqn8L3A/8Ztts2R2/PYClsQ+4msGJz1eA6ybbnEX1J8AZwP7WE3qgqn6nqg4l2cPgux9OADdU1WsTbOei6Ph2J+8DPgg8nuTRVvs4cCuwJ8n1wLPA+yfUvkn5GLA7yR8C32AQksuGVwJLUqccApKkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR16v8DvMbz9PTikSoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: -0.038410954520667805\n",
      "Standard Devation: 9.991669662907558\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "population = np.random.normal(0, 10, size=100000)\n",
    "\n",
    "bins = 100\n",
    "plt.hist(population, bins=bins)\n",
    "plt.show()\n",
    "print(\"Mean:\", np.mean(population))\n",
    "print(\"Standard Devation:\", np.std(population))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now assume we can only take samples of this population and that the financial cost of extracting the next sample increases overtime.  So let's say that the cost increases linearly with each sample extracted, by a rate of 2 cents more per sample.  Let's see how cost increases as we draw more samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_size = np.linspace(0, 10000)\n",
    "cost_for_sample_i = sample_size ** 2\n",
    "\n",
    "plt.plot(sample_size, cost_for_sample_i)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that the 10,000th sample costs 10,000,000 dollars in total!!!  That's a huge cost!  And that's at a rate of increase of 2 cents more per sample!  So these things can get very, very pricey.  The cost associated with obtaining a large sample or obtaining the entire population can be prohibitive.  If you think this example is an overstatement of reality, the 2010 census cost 13 billion dollars.  That's approximately 100 times more expensive than our worked example.  \n",
    "\n",
    "Hopefully I've motivated sampling for you and it's importance.  Now let's go through some techniques:  \n",
    "\n",
    "The first is just a random sample of the population distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: -0.7528969913432664\n",
      "Standard Devation: 9.8871479957865\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "sample_size = 100\n",
    "sample = np.random.choice(population, size=sample_size, replace=False)\n",
    "bins = 50\n",
    "plt.hist(sample, bins=bins)\n",
    "plt.show()\n",
    "print(\"Mean:\", np.mean(sample))\n",
    "print(\"Standard Devation:\", np.std(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few things to call out here:\n",
    "\n",
    "1) The sample is much less bell shaped than our population.  This is to be expected, since there is a lot less data and therefore the shape of our distribution is a lot less smooth.\n",
    "\n",
    "2) The population mean and the sample mean are close.  This is exactly what we'd hope for from a representative sample, our descriptive statistics aren't that far off from one another.  Recall, statistics like the mean are sensitive to outliers, which in some cases are a negative, but here that sensitivity actually helps ensure that we are as close as possible.  Of course, you should still look out for outliers, because if your sample is made up mostly of outliers, it may be the case that your sample \"appears\" good from your descriptive statistics, when in reality, your data is not representative.  That is why it's a good idea to capture multiple measures of center and spread and ensure they all align.\n",
    "\n",
    "3) The population standard deviation and sample standard deviation are close.  The fact that the spread about the center is similar is a good sign.  Unfortunately, the standard deviation is also not great at dealing with outliers, so it's best to include other statistics as well to compensate for this.\n",
    "\n",
    "4) We chose to sample without replacement, via `replace=False`.  This means once we've drawn a sample from our population we cannot draw that sample again.  Sampling with and without replacement is a huge decision in modeling.  If you sample with replacement then it's possible your model could just memorize the data and have terrible generalization.  So it's important, if you do sample with replacement that you truly need to.\n",
    "\n",
    "Now that we have seen _a_ sampling technique in action, let's apply it to our problem.  We are going to do the following things:\n",
    "\n",
    "1) write a function that samples from our training data (not our testing data)\n",
    "2) this function must create samples that satisify the descriptive statistics of our original dataset (as close as we can get them)\n",
    "3) we will specifically be upsampling, aka sampling with replacement from the minority class.  And taking a random sample from our majority class.  By down sampling, aka sampling without replacement, from our majority class we need to upsample less from our minority class.  \n",
    "4) We'll try different levels of upsampling and downsampling such that our test set accuracy is maximized.  \n",
    "5) finally we'll look at results on a validation set of previously unseen examples to make sure everything wasn't overtuned for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import iqr\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "def trimean(arr):\n",
    "    q1 = np.percentile(arr, 25)\n",
    "    q3 = np.percentile(arr, 75)\n",
    "    median = np.percentile(arr, 50)\n",
    "    return (q1 + 2*median + q3)/4\n",
    "\n",
    "def compare_categorical_indices(population_value_proportions, sample_index, population_index, missing_tolerance, include_outliers):\n",
    "    if include_outliers:\n",
    "        if sample_index != population_index:\n",
    "            object_outside_tolerance = True\n",
    "        else:\n",
    "            object_outside_tolerance = False\n",
    "    else:\n",
    "        majority_population_indices = []\n",
    "        for index in population_index:\n",
    "            if population_value_proportions[index] > missing_tolerance:\n",
    "                majority_population_indices.append(index)\n",
    "        sample_index_list = list(sample_index)\n",
    "        sample_index_list.sort()\n",
    "        majority_population_indices.sort()\n",
    "        if sample_index_list == majority_population_indices:\n",
    "            object_outside_tolerance = True\n",
    "        else:\n",
    "            object_outside_tolerance = False\n",
    "    return object_outside_tolerance\n",
    "\n",
    "def compare_categorical_statistic(tolerance, population_value_proportions, sample_value_proportions, sample_index):\n",
    "    population_proportions_in_sample = population_value_proportions[sample_index]\n",
    "    proportional_differences = population_proportions_in_sample - sample_value_proportions\n",
    "    delta = 0\n",
    "    outside_tolerance = False\n",
    "    for proportion_difference in proportional_differences:\n",
    "        if proportion_difference > tolerance:\n",
    "            outside_tolerance = True\n",
    "        delta += abs(proportion_difference)\n",
    "    return outside_tolerance, delta\n",
    "\n",
    "def compare_categorical(df, sample, column, tolerance, missing_tolerance, include_outliers):\n",
    "    population_value_proportions = df[column].value_counts()/len(df)\n",
    "    sample_value_proportions = sample[column].value_counts()/len(sample)\n",
    "    \n",
    "    sample_index = sample_value_proportions.index.sort_values()\n",
    "    population_index = population_value_proportions.index.sort_values()\n",
    "    \n",
    "    outside_tolerance_indices = compare_categorical_indices(\n",
    "        population_value_proportions, sample_index, population_index, \n",
    "        missing_tolerance, include_outliers\n",
    "    )\n",
    "    outside_tolerance_statistic, delta = compare_categorical_statistic(\n",
    "        tolerance, population_value_proportions, sample_value_proportions, sample_index\n",
    "    )\n",
    "    outside_tolerance = outside_tolerance_indices and outside_tolerance_statistic\n",
    "    return outside_tolerance, delta\n",
    "    \n",
    "def prune_outliers(df, sample, column):\n",
    "    forest = IsolationForest(n_estimators=10, warm_start=False)\n",
    "    forest.fit(df[column].values.reshape(-1, 1))\n",
    "    is_inlier_population = forest.predict(df[column].values.reshape(-1, 1))\n",
    "    is_inlier_population = pd.Series(is_inlier_population)\n",
    "    is_inlier_population.index = df.index\n",
    "    is_inlier_population = is_inlier_population.map({1: True, -1: False})\n",
    "    population_column = df[column][is_inlier_population]\n",
    "    is_inlier_sample = forest.predict(sample[column].values.reshape(-1, 1))\n",
    "    is_inlier_sample = pd.Series(is_inlier_sample)\n",
    "    is_inlier_sample.index = sample.index\n",
    "    is_inlier_sample = is_inlier_sample.map({1: True, -1: False})\n",
    "    sample_column = sample[column][is_inlier_sample]\n",
    "    return population_column, sample_column \n",
    "    \n",
    "def compare_continuous(df, sample, column, tolerance, include_outliers):\n",
    "    mean_outside_tolerance = False\n",
    "    trimean_outside_tolerance = False\n",
    "    stdev_outside_tolerance = False\n",
    "    iqr_outside_tolerance = False\n",
    "    \n",
    "    if not include_outliers:\n",
    "        population_column, sample_column = prune_outliers(df, sample, column)\n",
    "    else:\n",
    "        population_column, sample_column = df[column], sample[column]\n",
    "        \n",
    "    mean_deviation = abs(population_column.mean() - sample_column.mean())\n",
    "    trimean_deviation = abs(trimean(population_column) - trimean(sample_column))\n",
    "    stdev_deviation = abs(population_column.std() - sample_column.std())\n",
    "    iqr_deviation = abs(iqr(population_column) - iqr(sample_column))\n",
    "    \n",
    "    if mean_deviation > tolerance:\n",
    "        mean_outside_tolerance = True\n",
    "    if trimean_deviation > tolerance:\n",
    "        trimean_outside_tolerance = True\n",
    "    if stdev_deviation > tolerance:\n",
    "        stdev_outside_tolerance = True\n",
    "    if iqr_deviation > tolerance:\n",
    "        iqr_outside_tolerance = True\n",
    "    delta = mean_deviation + trimean_deviation + stdev_deviation + iqr_deviation\n",
    "    outside_tolerance = mean_outside_tolerance and trimean_outside_tolerance \n",
    "    outside_tolerance = outside_tolerance and stdev_outside_tolerance\n",
    "    outside_tolerance = outside_tolerance and iqr_outside_tolerance\n",
    "    return outside_tolerance, delta\n",
    "    \n",
    "def compare_columns(df, sample, column, column_type,\n",
    "                    tolerance, outside_tolerance, \n",
    "                    missing_tolerance, include_outliers):\n",
    "    if column_type == \"categorical\":\n",
    "        categorical_outside_tolerance, delta = compare_categorical(\n",
    "            df, sample, column, tolerance, \n",
    "            missing_tolerance, include_outliers\n",
    "        )\n",
    "        outside_tolerance = outside_tolerance and categorical_outside_tolerance\n",
    "    if column_type == \"continuous\":\n",
    "        continuous_outside_tolerance, delta = compare_continuous(\n",
    "            df, sample, column, tolerance, include_outliers\n",
    "        )\n",
    "        outside_tolerance = outside_tolerance and continuous_outside_tolerance\n",
    "    return outside_tolerance, delta\n",
    "        \n",
    "def bootstrap(df, column_types, size=1000, tolerance=0.1, \n",
    "              max_iter=1000, missing_tolerance=0.1, \n",
    "              with_replacement=False, include_outliers=False):\n",
    "    outside_tolerance = True\n",
    "    best_sample = None\n",
    "    cur_iter = 0\n",
    "    if max_iter != -1:\n",
    "        limited_iter = True\n",
    "    else:\n",
    "        limited_iter = False\n",
    "    while outside_tolerance:\n",
    "        sample = df.sample(n=size, replace=with_replacement)\n",
    "        overall_delta = 0\n",
    "        min_overall_delta = 1e10\n",
    "        for column in df.columns:\n",
    "            column_type = column_types[column]\n",
    "            outside_tolerance, marginal_delta = compare_columns(\n",
    "                df, sample, column, column_type, \n",
    "                tolerance, outside_tolerance,\n",
    "                missing_tolerance, include_outliers\n",
    "            )\n",
    "            overall_delta += marginal_delta\n",
    "        if not outside_tolerance:\n",
    "            best_sample = sample\n",
    "        elif overall_delta < min_overall_delta:\n",
    "            min_overall_delta = overall_delta\n",
    "            best_sample = sample\n",
    "        cur_iter += 1\n",
    "        if limited_iter and cur_iter == max_iter:\n",
    "            break\n",
    "    return best_sample\n",
    "\n",
    "column_types = {\n",
    "    \"CreditScore\": \"continuous\",\n",
    "    \"Age\": \"continuous\",\n",
    "    \"Tenure\": \"continuous\",\n",
    "    \"Balance\": \"continuous\",\n",
    "    \"NumOfProducts\": \"categorical\",\n",
    "    \"IsActiveMember\": \"categorical\",\n",
    "    \"EstimatedSalary\": \"continuous\",\n",
    "    \"Churn\": \"categorical\",\n",
    "    \"France\": \"categorical\",\n",
    "    \"Germany\": \"categorical\",\n",
    "    \"Spain\": \"categorical\",\n",
    "    \"Female\": \"categorical\",\n",
    "    \"Male\": \"categorical\"\n",
    "}\n",
    "sample = bootstrap(df, column_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run our bootstrap to create a sample, just to confirm the representativeness of our sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Churn</th>\n",
       "      <th>France</th>\n",
       "      <th>Germany</th>\n",
       "      <th>Spain</th>\n",
       "      <th>Female</th>\n",
       "      <th>Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>652.532000</td>\n",
       "      <td>39.304000</td>\n",
       "      <td>5.013000</td>\n",
       "      <td>78722.653150</td>\n",
       "      <td>1.595000</td>\n",
       "      <td>0.544000</td>\n",
       "      <td>98596.245520</td>\n",
       "      <td>0.185000</td>\n",
       "      <td>0.486000</td>\n",
       "      <td>0.268000</td>\n",
       "      <td>0.246000</td>\n",
       "      <td>0.466000</td>\n",
       "      <td>0.534000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>96.292893</td>\n",
       "      <td>10.796673</td>\n",
       "      <td>2.908486</td>\n",
       "      <td>63429.830282</td>\n",
       "      <td>0.572103</td>\n",
       "      <td>0.498309</td>\n",
       "      <td>56992.503264</td>\n",
       "      <td>0.388492</td>\n",
       "      <td>0.500054</td>\n",
       "      <td>0.443139</td>\n",
       "      <td>0.430894</td>\n",
       "      <td>0.499092</td>\n",
       "      <td>0.499092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>351.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>91.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>583.500000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50317.887500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>656.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>98629.800000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>96618.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>721.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>129767.917500</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>145189.810000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>850.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>209490.210000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>199505.530000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CreditScore          Age       Tenure        Balance  NumOfProducts  \\\n",
       "count  1000.000000  1000.000000  1000.000000    1000.000000    1000.000000   \n",
       "mean    652.532000    39.304000     5.013000   78722.653150       1.595000   \n",
       "std      96.292893    10.796673     2.908486   63429.830282       0.572103   \n",
       "min     351.000000    18.000000     0.000000       0.000000       1.000000   \n",
       "25%     583.500000    32.000000     2.000000       0.000000       1.000000   \n",
       "50%     656.000000    38.000000     5.000000   98629.800000       2.000000   \n",
       "75%     721.000000    45.000000     8.000000  129767.917500       2.000000   \n",
       "max     850.000000    79.000000    10.000000  209490.210000       4.000000   \n",
       "\n",
       "       IsActiveMember  EstimatedSalary        Churn       France      Germany  \\\n",
       "count     1000.000000      1000.000000  1000.000000  1000.000000  1000.000000   \n",
       "mean         0.544000     98596.245520     0.185000     0.486000     0.268000   \n",
       "std          0.498309     56992.503264     0.388492     0.500054     0.443139   \n",
       "min          0.000000        91.750000     0.000000     0.000000     0.000000   \n",
       "25%          0.000000     50317.887500     0.000000     0.000000     0.000000   \n",
       "50%          1.000000     96618.600000     0.000000     0.000000     0.000000   \n",
       "75%          1.000000    145189.810000     0.000000     1.000000     1.000000   \n",
       "max          1.000000    199505.530000     1.000000     1.000000     1.000000   \n",
       "\n",
       "             Spain       Female         Male  \n",
       "count  1000.000000  1000.000000  1000.000000  \n",
       "mean      0.246000     0.466000     0.534000  \n",
       "std       0.430894     0.499092     0.499092  \n",
       "min       0.000000     0.000000     0.000000  \n",
       "25%       0.000000     0.000000     0.000000  \n",
       "50%       0.000000     0.000000     1.000000  \n",
       "75%       0.000000     1.000000     1.000000  \n",
       "max       1.000000     1.000000     1.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Churn</th>\n",
       "      <th>France</th>\n",
       "      <th>Germany</th>\n",
       "      <th>Spain</th>\n",
       "      <th>Female</th>\n",
       "      <th>Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>650.528800</td>\n",
       "      <td>38.921800</td>\n",
       "      <td>5.012800</td>\n",
       "      <td>76485.889288</td>\n",
       "      <td>1.530200</td>\n",
       "      <td>0.515100</td>\n",
       "      <td>100090.239881</td>\n",
       "      <td>0.203700</td>\n",
       "      <td>0.501400</td>\n",
       "      <td>0.250900</td>\n",
       "      <td>0.247700</td>\n",
       "      <td>0.454300</td>\n",
       "      <td>0.545700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>96.653299</td>\n",
       "      <td>10.487806</td>\n",
       "      <td>2.892174</td>\n",
       "      <td>62397.405202</td>\n",
       "      <td>0.581654</td>\n",
       "      <td>0.499797</td>\n",
       "      <td>57510.492818</td>\n",
       "      <td>0.402769</td>\n",
       "      <td>0.500023</td>\n",
       "      <td>0.433553</td>\n",
       "      <td>0.431698</td>\n",
       "      <td>0.497932</td>\n",
       "      <td>0.497932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>350.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.580000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>584.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51002.110000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>652.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>97198.540000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100193.915000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>718.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>127644.240000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>149388.247500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>850.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>250898.090000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>199992.480000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        CreditScore           Age        Tenure        Balance  NumOfProducts  \\\n",
       "count  10000.000000  10000.000000  10000.000000   10000.000000   10000.000000   \n",
       "mean     650.528800     38.921800      5.012800   76485.889288       1.530200   \n",
       "std       96.653299     10.487806      2.892174   62397.405202       0.581654   \n",
       "min      350.000000     18.000000      0.000000       0.000000       1.000000   \n",
       "25%      584.000000     32.000000      3.000000       0.000000       1.000000   \n",
       "50%      652.000000     37.000000      5.000000   97198.540000       1.000000   \n",
       "75%      718.000000     44.000000      7.000000  127644.240000       2.000000   \n",
       "max      850.000000     92.000000     10.000000  250898.090000       4.000000   \n",
       "\n",
       "       IsActiveMember  EstimatedSalary         Churn        France  \\\n",
       "count    10000.000000     10000.000000  10000.000000  10000.000000   \n",
       "mean         0.515100    100090.239881      0.203700      0.501400   \n",
       "std          0.499797     57510.492818      0.402769      0.500023   \n",
       "min          0.000000        11.580000      0.000000      0.000000   \n",
       "25%          0.000000     51002.110000      0.000000      0.000000   \n",
       "50%          1.000000    100193.915000      0.000000      1.000000   \n",
       "75%          1.000000    149388.247500      0.000000      1.000000   \n",
       "max          1.000000    199992.480000      1.000000      1.000000   \n",
       "\n",
       "            Germany         Spain        Female          Male  \n",
       "count  10000.000000  10000.000000  10000.000000  10000.000000  \n",
       "mean       0.250900      0.247700      0.454300      0.545700  \n",
       "std        0.433553      0.431698      0.497932      0.497932  \n",
       "min        0.000000      0.000000      0.000000      0.000000  \n",
       "25%        0.000000      0.000000      0.000000      0.000000  \n",
       "50%        0.000000      0.000000      0.000000      1.000000  \n",
       "75%        1.000000      0.000000      1.000000      1.000000  \n",
       "max        1.000000      1.000000      1.000000      1.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 13), (1000, 13))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape, sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see all of the proportions are pretty close for all the variables!  Also note, that our sample is a 10th the size from our original!!!  We are now in a place to generate our upsample for the minor class and our down sample for the majority class.\n",
    "\n",
    "We do this with the following steps:\n",
    "\n",
    "1. split into test, train and validation - \n",
    "\n",
    "2. we upsample and down sample on train and tune over test.  \n",
    "\n",
    "3. Then we validate on validation.  We should only validate once!\n",
    "\n",
    "## Don't Balance Test or Validation Data EVER\n",
    "\n",
    "Before we go through the code, I just want to make this point very explicit.  It is _incredibly_ important that we only mess around with our training data and not our testing or validation data.  This is because our testing and validation data should resemble the real world.  The world in which the data is unbalanced.  We cannot and should not assume that eventually there will be a world in which our classes are balanced.  So if we make our testing or validation data set balanced, any metrics we report will be wrong, because they are based on a false assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4005\n",
       "1     995\n",
       "Name: Churn, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Churn\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.72      0.81      2002\n",
      "           1       0.39      0.70      0.50       498\n",
      "\n",
      "    accuracy                           0.72      2500\n",
      "   macro avg       0.65      0.71      0.65      2500\n",
      "weighted avg       0.80      0.72      0.74      2500\n",
      "\n",
      "0.712539468563565\n",
      "9.684849452245544\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.73      0.81      1956\n",
      "           1       0.42      0.69      0.52       544\n",
      "\n",
      "    accuracy                           0.72      2500\n",
      "   macro avg       0.66      0.71      0.66      2500\n",
      "weighted avg       0.79      0.72      0.74      2500\n",
      "\n",
      "0.7121526524720317\n",
      "9.532869240938773\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    classification_report, roc_auc_score, log_loss, \n",
    "    precision_score, recall_score, f1_score\n",
    ")\n",
    "\n",
    "def resample_data(training_data, minority_size, majority_size):\n",
    "    minority = training_data[training_data[\"Churn\"] == 1]\n",
    "    majority = training_data[training_data[\"Churn\"] == 0]\n",
    "    \n",
    "    column_types = {\n",
    "        \"CreditScore\": \"continuous\",\n",
    "        \"Age\": \"continuous\",\n",
    "        \"Tenure\": \"continuous\",\n",
    "        \"Balance\": \"continuous\",\n",
    "        \"NumOfProducts\": \"categorical\",\n",
    "        \"IsActiveMember\": \"categorical\",\n",
    "        \"EstimatedSalary\": \"continuous\",\n",
    "        \"Churn\": \"categorical\",\n",
    "        \"France\": \"categorical\",\n",
    "        \"Germany\": \"categorical\",\n",
    "        \"Spain\": \"categorical\",\n",
    "        \"Female\": \"categorical\",\n",
    "        \"Male\": \"categorical\"\n",
    "    }\n",
    "\n",
    "    minority_sample = bootstrap(minority, column_types, size=minority_size, with_replacement=True)\n",
    "    majority_sample = bootstrap(majority, column_types, size=majority_size, with_replacement=False)\n",
    "    new_train = minority_sample.append(majority_sample)\n",
    "    for _ in range(10):\n",
    "        new_train = new_train.sample(frac=1.0)\n",
    "    return new_train\n",
    "\n",
    "# step one\n",
    "y = df[\"Churn\"]\n",
    "cols = df.columns.tolist()\n",
    "cols.remove(\"Churn\")\n",
    "X = df[cols]\n",
    "\n",
    "# split into 50/50 test train\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=12, test_size=.5\n",
    ")\n",
    "\n",
    "# split test into 50/50 test validation\n",
    "X_test, X_val, y_test, y_val = train_test_split(\n",
    "    X_test, y_test, random_state=12, test_size=.5\n",
    ")\n",
    "\n",
    "train = X_train.copy()\n",
    "train[\"Churn\"] = y_train.copy()\n",
    "\n",
    "minority_sizes = [\n",
    "    3000, 2000, 1000,\n",
    "    1000, 1000, \n",
    "    3000, 3000,\n",
    "    3000, 2000, 1000\n",
    "]\n",
    "majority_sizes = [\n",
    "    3000, 2000, 1000,\n",
    "    3000, 2000,\n",
    "    1000, 2000,\n",
    "    4000, 4000, 4000\n",
    "    \n",
    "]\n",
    "sizes = zip(minority_sizes, majority_sizes)\n",
    "best_overall_score = 0\n",
    "best_y_pred = None\n",
    "best_classifier = None\n",
    "for minority_size, majority_size in sizes:\n",
    "    overall_score = []\n",
    "    new_train = resample_data(\n",
    "        train, minority_size, majority_size\n",
    "    )\n",
    "\n",
    "    y_train = new_train[\"Churn\"]\n",
    "    cols = new_train.columns.tolist()\n",
    "    cols.remove(\"Churn\")\n",
    "    X_train = new_train[cols]\n",
    "\n",
    "    logit_linear = LogisticRegression(\n",
    "        solver=\"liblinear\",\n",
    "        C=1, penalty=\"l1\", max_iter=10000, \n",
    "    )\n",
    "    logit_linear.fit(X_train, y_train)\n",
    "    y_pred = logit_linear.predict(X_test)\n",
    "    overall_score.append(\n",
    "        precision_score(y_test, y_pred)\n",
    "    )\n",
    "    overall_score.append(\n",
    "        recall_score(y_test, y_pred)\n",
    "    )\n",
    "    overall_score.append(\n",
    "        roc_auc_score(y_test, y_pred)\n",
    "    )\n",
    "    overall_score.append(\n",
    "        f1_score(y_test, y_pred)\n",
    "    )\n",
    "    overall_score = np.mean(overall_score)\n",
    "    if overall_score > best_overall_score:\n",
    "        best_overall_score = overall_score\n",
    "        best_y_pred = y_pred\n",
    "        best_classifier = logit_linear\n",
    "    \n",
    "print(classification_report(y_test, best_y_pred))\n",
    "print(roc_auc_score(y_test, best_y_pred))\n",
    "print(log_loss(y_test, best_y_pred))\n",
    "\n",
    "y_pred_val = best_classifier.predict(X_val)\n",
    "print(classification_report(y_val, y_pred_val))\n",
    "print(roc_auc_score(y_val, y_pred_val))\n",
    "print(log_loss(y_val, y_pred_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old score:\n",
    "```\n",
    "           precision    recall  f1-score   support\n",
    "\n",
    "           0       0.89      0.72      0.80      1944\n",
    "           1       0.42      0.70      0.53       556\n",
    "\n",
    "    accuracy                           0.72      2500\n",
    "   macro avg       0.66      0.71      0.66      2500\n",
    "weighted avg       0.79      0.72      0.74      2500\n",
    "\n",
    "0.7127298161470823\n",
    "9.740107656409736\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, in this case our naive bootstrap sampling algorithm doesn't do any better.  It makes the data a little bit better for class 0 (no churn), but a little worse for class 1 (churn) on the test set.  Since the changes are so small, this could honestly just be because of a reduction in training size.  The classifier does a little bit better on the validation set, but that might just be how easy it was to classify that section of the data.  We can't draw any real conclusions about increased accuracy.\n",
    "\n",
    "So is sampling useful for classification?  The answer is possibly!  We didn't do much better with our naive sampler, but now let's make use of two sampling techniques built for this type of problem, rather than a generic sampler:\n",
    "\n",
    "* Synthetic Minority Oversampling Technique - SMOTE\n",
    "* Adaptive Synthetic sampling method - ADASYN\n",
    "\n",
    "I won't go super into the details, but these two samplers are part of `imbalanced-learn` a package developed just to deal with imbalanced data.\n",
    "\n",
    "Let's see how these two algorithms do with our problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.91      0.88      1564\n",
      "           1       0.56      0.39      0.46       436\n",
      "\n",
      "    accuracy                           0.80      2000\n",
      "   macro avg       0.70      0.65      0.67      2000\n",
      "weighted avg       0.78      0.80      0.79      2000\n",
      "\n",
      "0.6547285248363406\n",
      "6.856000287618878\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "y = df[\"Churn\"]\n",
    "cols = df.columns.tolist()\n",
    "cols.remove(\"Churn\")\n",
    "X = df[cols]\n",
    "\n",
    "smote = SMOTE()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=12, test_size=0.2\n",
    ")\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "logit_linear = LogisticRegression(\n",
    "    solver=\"liblinear\",\n",
    "    C=1, penalty=\"l1\", max_iter=10000, \n",
    ")\n",
    "logit_linear.fit(X_train, y_train)\n",
    "y_pred = logit_linear.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(roc_auc_score(y_test, y_pred))\n",
    "print(log_loss(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.92      0.88      1564\n",
      "           1       0.57      0.39      0.46       436\n",
      "\n",
      "    accuracy                           0.80      2000\n",
      "   macro avg       0.71      0.65      0.67      2000\n",
      "weighted avg       0.78      0.80      0.79      2000\n",
      "\n",
      "0.6535260095262676\n",
      "6.786919936238052\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "y = df[\"Churn\"]\n",
    "cols = df.columns.tolist()\n",
    "cols.remove(\"Churn\")\n",
    "X = df[cols]\n",
    "\n",
    "adasyn = ADASYN()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=12, test_size=0.2\n",
    ")\n",
    "X_train, y_train = adasyn.fit_resample(X_train, y_train)\n",
    "\n",
    "logit_linear = LogisticRegression(\n",
    "    solver=\"liblinear\",\n",
    "    C=1, penalty=\"l1\", max_iter=10000, \n",
    "    class_weight=\"balanced\"\n",
    ")\n",
    "logit_linear.fit(X_train, y_train)\n",
    "y_pred = logit_linear.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(roc_auc_score(y_test, y_pred))\n",
    "print(log_loss(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we do a bit better on class 0, but worse on class 1.  \n",
    "\n",
    "**One important thing to note:** _We only resample on training data, never on testing_.  \n",
    "\n",
    "If we resample on testing data then we shouldn't trust our results, because it doesn't actually match the real world anymore.  Imbalanced learn comes with some other tools and techniques.  Let's see if adding those improves things.  I know I said this already, but I'm repeating it so it sticks.\n",
    "\n",
    "First let's get a baseline from scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.95      0.91      1991\n",
      "           1       0.70      0.50      0.58       509\n",
      "\n",
      "    accuracy                           0.86      2500\n",
      "   macro avg       0.79      0.72      0.75      2500\n",
      "weighted avg       0.84      0.86      0.85      2500\n",
      "\n",
      "0.7226379217283276\n",
      "5.00124904475308\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = df[\"Churn\"]\n",
    "cols = df.columns.tolist()\n",
    "cols.remove(\"Churn\")\n",
    "X = df[cols]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=0\n",
    ")\n",
    "bc = BaggingClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(), \n",
    "    random_state=0\n",
    ")\n",
    "bc.fit(X_train, y_train) \n",
    "\n",
    "y_pred = bc.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(roc_auc_score(y_test, y_pred))\n",
    "print(log_loss(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we did better, so maybe a decision tree was actually the way to go all along.  Let's see if using the imbalanced bagger will make a difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.85      0.88      1991\n",
      "           1       0.54      0.68      0.60       509\n",
      "\n",
      "    accuracy                           0.82      2500\n",
      "   macro avg       0.73      0.77      0.74      2500\n",
      "weighted avg       0.84      0.82      0.82      2500\n",
      "\n",
      "0.7660498767045024\n",
      "6.313782357647464\n"
     ]
    }
   ],
   "source": [
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "\n",
    "bbc = BalancedBaggingClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(),\n",
    "    sampling_strategy='auto',\n",
    "    replacement=False,\n",
    "    random_state=0\n",
    ")\n",
    "bbc.fit(X_train, y_train) \n",
    "\n",
    "y_pred = bbc.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(roc_auc_score(y_test, y_pred))\n",
    "print(log_loss(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We definitely did better!  Looks like using the imbalanced bagging really helped out a lot!  But still lots of room for improvement.  Notice that the log loss went up by some.  Next let's try doing both a sampler and a balanced classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.92      0.90      1564\n",
      "           1       0.66      0.57      0.61       436\n",
      "\n",
      "    accuracy                           0.84      2000\n",
      "   macro avg       0.77      0.74      0.76      2000\n",
      "weighted avg       0.84      0.84      0.84      2000\n",
      "\n",
      "0.7438026467068679\n",
      "5.43990805663525\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "\n",
    "y = df[\"Churn\"]\n",
    "cols = df.columns.tolist()\n",
    "cols.remove(\"Churn\")\n",
    "X = df[cols]\n",
    "\n",
    "adasyn = ADASYN()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=12, test_size=0.2\n",
    ")\n",
    "\n",
    "X_train, y_train = adasyn.fit_resample(X_train, y_train)\n",
    "\n",
    "bbc = BalancedBaggingClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(),\n",
    "    sampling_strategy='auto',\n",
    "    replacement=False,\n",
    "    random_state=0\n",
    ")\n",
    "bbc.fit(X_train, y_train) \n",
    "\n",
    "y_pred = bbc.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(roc_auc_score(y_test, y_pred))\n",
    "print(log_loss(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the score from the logistic regression for comparison:\n",
    "\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.84      0.92      0.88      1564\n",
    "           1       0.57      0.39      0.46       436\n",
    "\n",
    "    accuracy                           0.80      2000\n",
    "   macro avg       0.71      0.65      0.67      2000\n",
    "weighted avg       0.78      0.80      0.79      2000\n",
    "\n",
    "0.6535260095262676\n",
    "6.786919936238052\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like the balanced decision tree does the best of what we've seen so far!  While we didn't do \"great\", sampling plus using a specialized classifier built for imbalanced data definitely _helps_.  Let's just try one more thing before we head onto anamoly detection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.93      0.91      1564\n",
      "           1       0.70      0.56      0.62       436\n",
      "\n",
      "    accuracy                           0.85      2000\n",
      "   macro avg       0.79      0.75      0.77      2000\n",
      "weighted avg       0.84      0.85      0.85      2000\n",
      "\n",
      "0.7465684319200356\n",
      "5.111780485513152\n"
     ]
    }
   ],
   "source": [
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = df[\"Churn\"]\n",
    "cols = df.columns.tolist()\n",
    "cols.remove(\"Churn\")\n",
    "X = df[cols]\n",
    "\n",
    "adasyn = ADASYN()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=12, test_size=0.2\n",
    ")\n",
    "\n",
    "X_train, y_train = adasyn.fit_resample(X_train, y_train)\n",
    "\n",
    "brf = BalancedRandomForestClassifier(n_estimators=100, random_state=0)\n",
    "brf.fit(X_train, y_train) \n",
    "y_pred = brf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(roc_auc_score(y_test, y_pred))\n",
    "print(log_loss(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This classifier definitely does the best!  We are almost to pretty solid classification, the only thing that's really lagging is the recall for class 1.  That means, we are only capturing 56% of the total number of cases of Churn.  At least, of those that we think are Churn, we are right 70% of the time!  Basically, it looks like our classifier is a little over eager in classifying folks as not churning.  But I think that's fine!  Since most folks won't churn.  This seems like the best we are going to do.  There is more stuff in the imbalanced-learn package we could try, like adding more metrics and trying different samplers.  But I leave that as an exercise to the reader.  We also didn't exhaust the list of possible classifiers, including using deep learning!!!  But we'll get to that in a later chapter.  Anywho, onto anamoly detection!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Anamoly Detection\n",
    "\n",
    "It's time for to deal with the problem that is the bane of every data scientists existance - anamoly detection.  If you've never dealt with one of these before, don't worry, at some point you will.  These things come up _a lot_ in information extraction from text.  But they also come up generally in a bunch of places.  Normal models won't work, no matter what you try.  And worse then that, you might not even realize you are dealing with an anamoly detection problem until it's far too late.  As in, you've gone to production and the sample data you saw for training and testing was _far_ too curated.  \n",
    "\n",
    "Anamoly detection, is basically when your data falls into a class around 90-95% and about 10% or less the rest of the time.  It may be the case that this happens because, like I stated above the data you saw when you were developing the model was _not_ representative.  This can get _especially_ bad with multiclass classification where multiple classes are relatively rare in production.  You might say, these cases are easy!  Just ignore the rare classes, no point in dredging the swamp for the extra little bit of value!  And honestly, that might be a good answer, _sometimes_.  But you won't always get lucky with who you are employed by, who your boss is, or what they care about.\n",
    "\n",
    "Or in some cases, it may be by the design of the problem.  Of course, the first scenario is _far_ worse, so we will focus on the second case, where there is an anamolous class, by design.  We will also focus on the case of binary classification, because multiclass classification, is frankly speaking, just too hard.\n",
    "\n",
    "### Problem Set up - Credit Card Fraud Detection\n",
    "\n",
    "The problem of fraud is fairly straight forward - someone uses your card for a purchase that's \"weird\" and the credit card company tells you about it.  Of course, this case isn't going to happen often (we hope).  So it's the minority class by _a lot_.  We might not have many examples per customer of this occurring.  However, we may have 'enough' examples over the course of many customers.  This of course, means there will be a lot more cases of transactions where this _didn't_ happen, which can be tough, because your classifier will just learn the majority class.  This also may not be reflected in your metrics since precision, recall and f1 score will all look _great_.  However, really what this means is, they are great for the majority class, not the minority class.\n",
    "\n",
    "### Our data\n",
    "\n",
    "Our data comes from this kaggle competition: https://www.kaggle.com/mlg-ulb/creditcardfraud\n",
    "\n",
    "Let's go ahead and do some exploration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      "Time      284807 non-null float64\n",
      "V1        284807 non-null float64\n",
      "V2        284807 non-null float64\n",
      "V3        284807 non-null float64\n",
      "V4        284807 non-null float64\n",
      "V5        284807 non-null float64\n",
      "V6        284807 non-null float64\n",
      "V7        284807 non-null float64\n",
      "V8        284807 non-null float64\n",
      "V9        284807 non-null float64\n",
      "V10       284807 non-null float64\n",
      "V11       284807 non-null float64\n",
      "V12       284807 non-null float64\n",
      "V13       284807 non-null float64\n",
      "V14       284807 non-null float64\n",
      "V15       284807 non-null float64\n",
      "V16       284807 non-null float64\n",
      "V17       284807 non-null float64\n",
      "V18       284807 non-null float64\n",
      "V19       284807 non-null float64\n",
      "V20       284807 non-null float64\n",
      "V21       284807 non-null float64\n",
      "V22       284807 non-null float64\n",
      "V23       284807 non-null float64\n",
      "V24       284807 non-null float64\n",
      "V25       284807 non-null float64\n",
      "V26       284807 non-null float64\n",
      "V27       284807 non-null float64\n",
      "V28       284807 non-null float64\n",
      "Amount    284807 non-null float64\n",
      "Class     284807 non-null int64\n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"creditcard.csv\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there a bunch of features prepended with a \"V\".  These are probably hidden pieces of information about individuals.  Of course, we don't want to snoop into anyone's personal lives, so we'll just take these features as a given.  Let's look at correlation between each of these features and the \"Class\" which is our label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Independent columns ['V19']\n",
      "Columns with no correlation ['V13', 'V15', 'V22', 'V23', 'V25', 'V26']\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "independent = []\n",
    "no_correlation = []\n",
    "for column in df.columns:\n",
    "    if column == \"Class\":\n",
    "        continue\n",
    "    label = df[\"Class\"]\n",
    "    if stats.kruskal(label, df[column]).pvalue > 0.01:\n",
    "        independent.append(column)\n",
    "    if stats.pointbiserialr(label, df[column]).pvalue > 0.01:\n",
    "        no_correlation.append(column)\n",
    "        \n",
    "print(\"Independent columns\", independent)\n",
    "print(\"Columns with no correlation\", no_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we can kick out some columns at the start.  But just to get a baseline, let's see how the a classifier does with all the columns being used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     71078\n",
      "           1       0.06      0.90      0.12       124\n",
      "\n",
      "    accuracy                           0.98     71202\n",
      "   macro avg       0.53      0.94      0.55     71202\n",
      "weighted avg       1.00      0.98      0.99     71202\n",
      "\n",
      "0.9400059362318\n",
      "0.8062240625839364\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score, log_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = df[\"Class\"].copy()\n",
    "columns = df.columns.tolist()\n",
    "columns.remove(\"Class\")\n",
    "X = df[columns].copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=12)\n",
    "\n",
    "logit_clf = LogisticRegression(\n",
    "    C=1, solver=\"liblinear\", penalty=\"l1\", \n",
    "    class_weight=\"balanced\", max_iter=10000\n",
    ")\n",
    "\n",
    "logit_clf.fit(X_train, y_train)\n",
    "y_pred = logit_clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(roc_auc_score(y_test, y_pred))\n",
    "print(log_loss(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may seem like we did mostly pretty good!  At least we did on class 0.  The real problem is what's happening in class 1, the minority class and unfortunately the one we care about getting right, a lot.\n",
    "\n",
    "So what does high recall, low precision mean?  \n",
    "\n",
    "Remember that precision is:\n",
    "\n",
    "$$ \\frac{true \\space positive}{true \\space positive + false \\space positive} $$\n",
    "\n",
    "This basically tells us of the things that we classified as fraud, how many of them actually were fraud.  So with this classifier, the number of times we actually found fraud was pretty low, but our classifier _thought_ we found it a lot.  Basically 94% of the time when it predicted \"this is fraud\", it wasn't fraud.  Ouch!  That sounds pretty bad!\n",
    "\n",
    "Let's look at the other side of the story:\n",
    "\n",
    "Remember that recall is:\n",
    "\n",
    "$$ \\frac{true \\space positive}{true \\space positive + false \\space negative} $$\n",
    "\n",
    "So the false negatives are the times when we thought we didn't have fraud but we actually did.  The ratio here can be a little bit more tricky, since we don't actually deal with negative cases.  I think this is what trips people up the most about recall - it just looks at the number of cases that were actually in the class.  So 90% of the cases in the test set that were actually class 1, were found by the classifier.  \n",
    "\n",
    "This means, that our classifier is over eager in classifying charges as fraud.  So one of the ways you can deal with this is by having a human verify charges.  Just have everyone that's classified as fraud sent to someone for verification.  This is what actually happens in the real world with most systems.  So are we done?  We've basically matched the performance of the systems that are actually in production, right?  Or at least somewhat close.  \n",
    "\n",
    "The next thing to do is try to improve things.  Let's start by kicking out those features we knew had no correlation or were independent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     71078\n",
      "           1       0.06      0.89      0.12       124\n",
      "\n",
      "    accuracy                           0.98     71202\n",
      "   macro avg       0.53      0.93      0.55     71202\n",
      "weighted avg       1.00      0.98      0.99     71202\n",
      "\n",
      "0.9318218331701021\n",
      "0.8154408029094835\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score, log_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "columns_to_remove = [\n",
    "    \"V19\", 'V13', 'V15', \n",
    "    'V22', 'V23', 'V25', \n",
    "    'V26', \"Class\"\n",
    "]\n",
    "y = df[\"Class\"].copy()\n",
    "columns = df.columns.tolist()\n",
    "for column_to_remove in columns_to_remove:\n",
    "    columns.remove(column_to_remove)\n",
    "X = df[columns].copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=12)\n",
    "\n",
    "logit_clf = LogisticRegression(\n",
    "    C=1, solver=\"liblinear\", penalty=\"l1\", \n",
    "    class_weight=\"balanced\", max_iter=10000\n",
    ")\n",
    "\n",
    "logit_clf.fit(X_train, y_train)\n",
    "y_pred = logit_clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(roc_auc_score(y_test, y_pred))\n",
    "print(log_loss(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we don't do any better.  Well, let's try our resampling strategy like last time, that might work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56869\n",
      "           1       0.56      0.81      0.66        93\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.78      0.90      0.83     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n",
      "0.9027158625454426\n",
      "0.046083237643766366\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "\n",
    "y = df[\"Class\"].copy()\n",
    "cols = df.columns.tolist()\n",
    "cols.remove(\"Class\")\n",
    "X = df[cols].copy()\n",
    "\n",
    "adasyn = ADASYN()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=12, test_size=0.2\n",
    ")\n",
    "X_train, y_train = adasyn.fit_resample(X_train, y_train)\n",
    "\n",
    "bbc = BalancedBaggingClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(),\n",
    "    sampling_strategy='auto',\n",
    "    replacement=False,\n",
    "    random_state=0\n",
    ")\n",
    "bbc.fit(X_train, y_train) \n",
    "\n",
    "y_pred = bbc.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(roc_auc_score(y_test, y_pred))\n",
    "print(log_loss(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this appears to work better!  Not perfect, but we got our precision _way_ up!!  Can we do even better than that?  It's time introduce a new topic that we've lightly touched up, that of outlier detection.  By treating the minority class as outliers, we may be able to more accurately predict the minority class.\n",
    "\n",
    "The naive approach to outlier detection is look at magnitude.  Consider the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "distribution = np.random.normal(0, 1, size=50)\n",
    "distribution = np.append(distribution, [100, 200, 300, -400, -500])\n",
    "plt.hist(distribution, bins=9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the data is mostly centered around one, but there are some outliers.  How might we detect these?  One way is to use the z-score.  Let's define this tool mathematically and then see how to use it:\n",
    "\n",
    "$$ \\frac{x - \\mu}{\\sigma} $$ \n",
    "\n",
    "Where $x$ := the individual element\n",
    "\n",
    "$ \\mu $ := the mean\n",
    "\n",
    "$ \\sigma $ := the standard deviation\n",
    "\n",
    "So the z-score is the element's distance from the mean, accounting for the distribution of all the points.  Let's see how this works for our example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n",
      "200.0\n",
      "300.0\n",
      "-400.0\n",
      "-500.0\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "for index, zscore in enumerate(stats.zscore(distribution)):\n",
    "    if abs(zscore) > 1.0:\n",
    "        print(distribution[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we were able to recover all the outliers!  What do we do when the data is multivariate?  Does the notion of the z-score scale?\n",
    "\n",
    "Turns out it does!  \n",
    "\n",
    "## Enter Mahalanobis Distance\n",
    "\n",
    "The Mahalanobis distance is a generalization of the z-score where instead of considering the distance from a point to the mean, we consider the distance between a point and distribution.  There are three big differences between Mahalanobis distance and z-score:\n",
    "\n",
    "1) we are dealing with vectors and matrices\n",
    "2) we use the covariance matrix as our normalization rather than the standard deviation\n",
    "3) since we have no global mean, we use the mean with respect to each variable in our vector\n",
    "\n",
    "Let's look at the equation:\n",
    "\n",
    "$$ D(x) = \\sqrt{(x - \\mu)^{T}S^{-1}(x- \\mu)} $$\n",
    "\n",
    "Here we define:\n",
    "\n",
    "$ x $ := an observation, such that:\n",
    "\n",
    "$$ x := (x_{1}, x_{2}, x_{3}, ... x_{n}) $$\n",
    "\n",
    "So $x_{i}$ represents the ith element in a given observation.\n",
    "\n",
    "$ \\mu $ := the mean for each of the positional elements of the observation, expressed as a vector such that:\n",
    "\n",
    "$$ \\mu := (\\mu_{1}, \\mu_{2}, \\mu_{3}, ... \\mu_{n}) $$\n",
    "\n",
    "So $ \\mu_{i}$ is the mean for the ith position across all the observations.\n",
    "\n",
    "$S$ := the covariance matrix for the multivariate distribution.\n",
    "\n",
    "This formalization of distance leads to the following conclusions about how a Mahalanobis distance behaves:\n",
    "\n",
    "1) It transforms the columns into uncorrelated variables because we normalize by the covariance as well as demean the data.\n",
    "2) Scale the columns to make their variance equal to 1.\n",
    "\n",
    "Now let's look at an implementation of Mahalanobis distance as a classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      5992\n",
      "           1       0.00      1.00      0.00         8\n",
      "\n",
      "    accuracy                           0.00      6000\n",
      "   macro avg       0.00      0.50      0.00      6000\n",
      "weighted avg       0.00      0.00      0.00      6000\n",
      "\n",
      "0.5\n",
      "34.49352322435109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from scipy import linalg\n",
    "import numpy as np\n",
    "\n",
    "def mahalanobis(x, distribution, covariance=None):\n",
    "    x_demeaned = x - np.mean(distribution)\n",
    "    if covariance is None:\n",
    "        covariance = np.cov(distribution.values.T)\n",
    "    inverse_covariance = linalg.inv(covariance)\n",
    "    left_term = np.dot(x_demeaned, inverse_covariance)\n",
    "    mahalanobis_distance = np.dot(left_term, x_demeaned.T)\n",
    "    return mahalanobis_distance.diagonal()\n",
    "\n",
    "class MahalanobisBinaryClassifier():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        class_one_index = y_train[y_train == 1].index\n",
    "        class_zero_index = y_train[y_train == 0].index\n",
    "        self.X_train_pos = X_train.loc[class_one_index]\n",
    "        self.X_train_neg = X_train.loc[class_zero_index]\n",
    "        self.positive_cov = np.cov(\n",
    "            self.X_train_pos.values.T\n",
    "        )\n",
    "        self.negative_cov = np.cov(\n",
    "            self.X_train_neg.values.T\n",
    "        )\n",
    "        \n",
    "    def predict_proba(self, X_test):\n",
    "        negative_distribution = mahalanobis(\n",
    "            X_test, self.X_train_neg, \n",
    "            covariance = self.positive_cov\n",
    "        )\n",
    "        positive_distribution = mahalanobis(\n",
    "            X_test, self.X_train_pos, \n",
    "            covariance = self.negative_cov\n",
    "        )\n",
    "        positive_negative_distribution = zip(\n",
    "            positive_distribution, negative_distribution\n",
    "        )\n",
    "        probabilities = []\n",
    "        for positive, negative in positive_negative_distribution:\n",
    "            denominator = positive + negative\n",
    "            probability_class_zero = 1 - (negative/denominator)\n",
    "            probability_class_one = 1 - (positive/denominator)\n",
    "            probabilities.append(\n",
    "                [probability_class_zero, probability_class_one]\n",
    "            ) \n",
    "        return np.array(probabilities)\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        predictions = []\n",
    "        for row in self.predict_proba(X_test):\n",
    "            predictions.append(np.argmax(row))\n",
    "        return np.array(predictions)\n",
    "\n",
    "sample_size = 30000\n",
    "column_types = {column: \"continuous\" for column in df.columns}\n",
    "sample = bootstrap(df, column_types, size=sample_size, with_replacement=False, include_outliers=True)\n",
    "\n",
    "y = sample[\"Class\"].copy()\n",
    "columns = sample.columns.tolist()\n",
    "columns.remove(\"Class\")\n",
    "X = sample[columns].copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=12\n",
    ")\n",
    "\n",
    "clf = MahalanobisBinaryClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(roc_auc_score(y_test, y_pred))\n",
    "print(log_loss(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For comparison here are the results of the sampling strategy:\n",
    "\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      1.00      1.00     56869\n",
    "           1       0.56      0.81      0.66        93\n",
    "\n",
    "    accuracy                           1.00     56962\n",
    "   macro avg       0.78      0.90      0.83     56962\n",
    "weighted avg       1.00      1.00      1.00     56962\n",
    "```\n",
    "\n",
    "And here are the results of our naive classifer:\n",
    "\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      0.98      0.99     71078\n",
    "           1       0.06      0.90      0.12       124\n",
    "\n",
    "    accuracy                           0.98     71202\n",
    "   macro avg       0.53      0.94      0.55     71202\n",
    "weighted avg       1.00      0.98      0.99     71202\n",
    "\n",
    "0.9400059362318\n",
    "0.8062240625839364\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it may seem like this is a bust, however it does manage to get 100% recall, which is definitely something!  It could be that the mahalanobis distance is simply too naive a strategy.  But given this high recall, there is definitely something to consider for outlier detection.  \n",
    "\n",
    "Also note, that we have to sample just because we run out of memory if we try to train on the full dataset.\n",
    "\n",
    "Next let's look at one of scikit-learn's outlier detection models - One Class SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM \n",
    "\n",
    "y = df[\"Class\"].copy()\n",
    "cols = df.columns.tolist()\n",
    "cols.remove(\"Class\")\n",
    "X = df[cols].copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2\n",
    ") \n",
    "class_zero_index = y_train[y_train == 0].index\n",
    "class_one_index = y_train[y_train == 1].index\n",
    "train_normal = X_train.loc[class_zero_index] \n",
    "train_outliers = X_train.loc[class_one_index] \n",
    "outlier_prop = len(train_outliers) / len(train_normal) \n",
    "svm = OneClassSVM(\n",
    "    kernel='rbf', nu=outlier_prop, gamma=0.000001\n",
    ") \n",
    "svm.fit(train_normal)\n",
    "y_pred = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "* https://towardsdatascience.com/hands-on-predict-customer-churn-5c2a42806266\n",
    "* https://blog.markgrowth.com/eliminating-churn-is-growth-hacking-2-0-47a380194a06\n",
    "* https://towardsdatascience.com/churn-prediction-3a4a36c2129a\n",
    "* https://www.profitwell.com/blog/the-complete-saas-guide-to-calculating-churn-rate-and-keeping-it-simple\n",
    "* https://www.machinelearningplus.com/statistics/mahalanobis-distance/\n",
    "* https://towardsdatascience.com/outlier-detection-with-one-class-svms-5403a1a1878c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To do:\n",
    "* normal distribution example with explicit outliers\n",
    "* zscore introduction and explanation\n",
    "* mahalanobis distance introduction and explanation\n",
    "* implemetn this: https://www.machinelearningplus.com/statistics/mahalanobis-distance/\n",
    "* one class svm example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
